---
title: "Linear Model: Model Selection"
output:
  prettydoc::html_pretty:
    theme: tactile 
    highlight: github
    math: katex
    toc: true
    self-contained: true
#date: "2025-03-11"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Model Selection for Linear Models

首先，以信用卡資料作為範例，讀取資料並整理成迴歸分析所需格式
```{r creDfRead, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
creUrl <- 'https://raw.githubusercontent.com/PingYangChen/BDA_Course_R_Code/refs/heads/main/sample_data/Credit.csv'
creDf <- read.csv(creUrl)
```

預覽欄位與少數前幾筆資料，檢視個欄位的資料型態。
```{r creDfPrev, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
names(creDf)
head(creDf, 4)
```

針對類別型變數，在 R 中請自行將欄位更改為 `factor` 型態。
```{r creDfSetProp, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
creDf$Own <- as.factor(creDf$Own)
creDf$Student <- as.factor(creDf$Student)
creDf$Married <- as.factor(creDf$Married)
creDf$Region <- as.factor(creDf$Region)
```

之後可做些視覺化觀察資料，例如對兩兩變數作圖。
```{r creDfPlot, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.align='center'}
library(ggplot2)
library(GGally)
ggpairs(creDf, aes(colour = "firebrick", alpha = 0.4)) 
```

## Stepwise Regression

逐步迴歸的執行方式：

1. 指定最小(無自變數)及最大(含所有自變數)模型

```{r creDfStepwisePrep, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
lm_null <- lm(Balance ~ 1, data = creDf)
lm_full <- lm(Balance ~ ., data = creDf)
```

2. 指定 Information Criterion 的參數
$$IC = -2\log{(Likelihood)} + k\times p$$
  - AIC：$k=2$
  - BIC：$k=\log{(n)}$

```{r creDfStepwiseCriVal, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# use AIC
aic_param <- 2
# use BIC
bic_param <- log(nrow(creDf)) # log(n)
```

3. 使用內建 `step` 函數執行 Stepwise Regression，以下以 BIC 選模為例

```{r creDfStepwise, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, progress=FALSE, results='hide'}
# forward method
mf <- step(lm_null, scope = list(lower = lm_null, upper = lm_full), 
           direction = "forward", k = bic_param)
# backward method
mb <- step(lm_full, scope = list(lower = lm_null, upper = lm_full), 
           direction = "backward", k = bic_param)
# both-way method
m2 <- step(lm_null, scope = list(lower = lm_null, upper = lm_full), 
           direction = "both", k = bic_param)
```

4. 檢視選模結果

```{r creDfStepwiseRes, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# summary(mf) # forward method
# summary(mb) # backward method
summary(m2) # both-way method
```

## Regularization Methods

### Ridge, Lasso and Elastic Net


`glmnet` 套件為 R 語言裡常用的 Regularization Method 套件

```{r loadglmnet, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
library(glmnet)
```

文件連結[點此](https://cran.r-project.org/web/packages/glmnet/index.html)

同樣以信用卡資料作為範例，先將資料整理成 `glmnet` 所需格式：`model.matrix`。

`glmnet` 未延續 R 語言的模型語法（如　`lm` 或 `glm` 使用之 `Y ~ x`），反而分別輸入反應變數（向量）及解釋變數（矩陣）。

- 反應變數可直接從資料中擷取。
- 解釋變數則透過 `model.matrix` 函數與 R 模型語法快速整理**（重點：記得刪除截距項）**。

```{r creDfModelMat, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Create Response Variable
y <- creDf$Balance
# Create model matrix
modelmat <- model.matrix(Balance ~ ., data = creDf)[,-1] # Remove intercept part
colnames(modelmat)
head(modelmat, 4)
```

`glmnet` 及 `cv.glmnet` 函數使用方法

- `alpha`: 控制 L1-penalty、L2-penalty 的分配比例，數值範圍為 [0, 1]。
  - `alpha = 0`，為 Ridge regression。
  - `alpha = 1`，為 Lasso regression。
  - `alpha` 介於 (0, 1) 之間，為 Elastic net regression，`alpha` 越大，L1-penalty 越重。
- `lambda`: 控制 L1-penalty、L2-penalty 的逞罰程度，`lambda` 越大，逞罰程度越重。
  - 一般而言，`lambda` 值可為一個由很小的值指數成長到一個大於 1 的值，例如 `exp(seq(-5, 1, length = 100))`。
- `standardize`: 是否對 `x` 進行標準化，基於解釋變數之間可能數值範圍、單位不同，預設為 `TRUE`。
- `intercept`: 是否配適截距項，預設為 `TRUE`。


給定一組 `lambda` 向量，可透過 `cv.glmnet` 函數進行交叉驗證選擇合適的 `lambda` 值，其關鍵輸出如下

- `lambda`: 使用者所輸入的 L1-penalty、L2-penalty 的逞罰程度向量。
- `cvm`: 交叉驗證的平均誤差，長度為 `length(lambda)`，分別對應每個 `lambda` 的驗證結果。
- `lambda.min`: 使 `cvm` 最小的 `lambda` 值，通常數值較小，逞罰效果較弱。
- `lambda.1se`: 交互驗證誤差距離最小 `cvm` 一單位標準差內的最大 `lambda` 值，可增加逞罰效果。


以 `cv.glmnet` 函數選擇合適的 `lambda` 值後，將挑選的 `lambda` 輸入 `glmnet` 建立最終模型。

```{r setseed, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
set.seed(2025)
```

以下為信用卡資料分析的示範程式

```{r creFitRidge, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Fit Ridge and tune lambda via CV
rid_cv <- cv.glmnet(modelmat, y, family = "gaussian", alpha = 0,
                    standardize = TRUE, intercept = TRUE, 
                    lambda = exp(seq(-5, 1, length = 100)))
# Visualize the tuning results
plot(rid_cv$lambda, rid_cv$cvm, type = "l", 
     xlab = "lambda", ylab = "cvm", main = "Ridge Reg.")
# Refit Ridge with proper lambda
m_rid <- glmnet(modelmat, y, family = "gaussian", alpha = 0,
                standardize = TRUE, intercept = TRUE,
                lambda = rid_cv$lambda.1se)
```
```{r creFitLasso, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Fit Lasso and tune lambda via CV
las_cv <- cv.glmnet(modelmat, y, family = "gaussian", alpha = 1,
                    standardize = TRUE, intercept = TRUE)
# Visualize the tuning results
nl <- 71
plot(las_cv$lambda[1:nl], las_cv$cvm[1:nl], type = "l", 
     xlab = "lambda", ylab = "cvm", main = "Lasso Reg.")
points(las_cv$lambda[1:nl], las_cv$cvup[1:nl], type = "l", lty = 2)
points(las_cv$lambda[1:nl], las_cv$cvlo[1:nl], type = "l", lty = 2)
# Refit Lasso with proper lambda
m_las <- glmnet(modelmat, y, family = "gaussian", alpha = 1,
                standardize = TRUE, intercept = TRUE,
                lambda = las_cv$lambda.1se)
```
```{r creFitEN, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Fit Elastic net and tune lambda via CV
en_cv <- cv.glmnet(modelmat, y, family = "gaussian", alpha = 0.3,
                   standardize = TRUE, intercept = TRUE)
# Visualize the tuning results
plot(en_cv$lambda, en_cv$cvm, type = "l", 
     xlab = "lambda", ylab = "cvm", main = "Elastic Net Reg.")
# Refit Elastic net with proper lambda
m_en <- glmnet(modelmat, y, family = "gaussian", alpha = 0.3,
               standardize = TRUE, intercept = TRUE,
               lambda = en_cv$lambda.1se)
```

比較 Ridge、Lasso、Elastic net $(\alpha=0.3)$ regression coefficients。

```{r glmnetCompare, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
regMethodResult <- cbind(
  rbind("(Intercept)" = m_rid$a0, m_rid$beta),
  rbind("(Intercept)" = m_las$a0, m_las$beta),
  rbind("(Intercept)" = m_en$a0, m_en$beta)
)
colnames(regMethodResult) <- c("ridge", "Lasso", "ElasN")
regMethodResult
```

### Group Lasso & Sparse Group Lasso

本節介紹兩種基於常用 Regularization Method 的延伸方法，與常用方法獨立對變數的迴歸係數進行壓縮不同，以下介紹的兩種方法為 Group Lasso 及 Sparse Group Lasso，使用上首先由使用者主觀方式將解釋變數分組，模型針對各組變數的迴歸係數加上逞罰效果，使整組變數一次被挑選出或剔除掉。


同樣以信用卡資料作為範例，除了各變數的主作用之外，再加入與 `Income` 相關的交互作用項: `Income:Age`、`Income:Education`、`Income:Own`、`Income:Student`。

先將資料依照需求整理成 `model.matrix` 格式。

```{r crePrepGL, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
modelmat2fi <- model.matrix(
  Balance ~ . + Income * (Age + Education + Own + Student), data = creDf
)[,-1] # Remove intercept from the model matrix
colnames(modelmat2fi)
```

手動將 `model.matrix` 的欄位分組，期望：

- 與 `Income` 相關的交互作用項及其主作用皆為同一組
- `Region`轉換出的兩個 Dummy variables 為同一組
- 其他變數各自為一組

```{r creGIDGL, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
twofi_group <- c(1, 2, 3, 4, 1, 
                 1, 1, 1, 5, 6, 
                 6, 1, 1, 1, 1)
names(twofi_group) <- colnames(modelmat2fi)
twofi_group
```

套件 **gglasso** 為 Group Lasso Regression 的建模套件，使用方法類似於 **glmnet** 套件，差別在於需要在 `group` 項輸入變數分組。

文件連結[點此](https://cran.r-project.org/web/packages/gglasso/index.html)

```{r loadgglasso, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
library(gglasso)
```

以下為使用 Group Lasso 分析信用卡資料內指定交互作用項的示範程式

```{r creFitGL, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Group Lasso: choose best lambda
glas_cv <- cv.gglasso(modelmat2fi, y, group = twofi_group, loss = "ls")
# Refit Group Lasso with proper lambda
m_glas <- gglasso(modelmat2fi, y, group = twofi_group, loss = "ls",
                  lambda = glas_cv$lambda.1se)
```

**Sparse Group Lasso** 相較於 Group Lasso 多了在同一組變數內增加 L1-penalty ，使得模型除了對各組變數的迴歸係數進行壓縮外，同時也組內的各變數進行迴歸係數壓縮，因此，Sparse Group Lasso 首先判斷一整組變數是否為關鍵變數組，再從組內挑出關鍵變數。

文件連結[點此](https://cran.r-project.org/web/packages/sparsegl/index.html)

使用方法類似 **gglasso** 及 **glmnet** 套件，惟須要設定 `asparse` L1-penalty 的懲罰程度，`asparse` 越大，表示組內變數迴歸係數的壓縮程度越大。

```{r loadSGL, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
library(sparsegl)
```

以下為使用 Sparse Group Lasso 分析信用卡資料內指定交互作用項的示範程式

```{r creFitSGL, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Sparse Group Lasso: choose best lambda using CV
sglas_cv <- cv.sparsegl(modelmat2fi, y, group = twofi_group, family = "gaussian",
                        asparse = 0.05) # set weight of L1-penalty as 0.05
# Refit Sparse Group Lasso with proper lambda
m_sglas <- sparsegl(modelmat2fi, y, group = twofi_group, family = "gaussian",
                    lambda = sglas_cv$lambda.1se, asparse = 0.05)
```


比較 Group Lasso、Sparse Group Lasso $(\alpha=0.05)$ regression coefficients。

```{r ggCompare, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
advRegMethodResult <- cbind(
  rbind("(Intercept)" = m_glas$b0, m_glas$beta),
  rbind("(Intercept)" = m_sglas$b0, m_sglas$beta)
)
colnames(advRegMethodResult) <- c("GroupLasso", "SGLasso")
advRegMethodResult
```

## Sure Independence Screening for Ultrahigh Dimensional Data

套件 **SIS** 為 Sure Independence Screening 的實作套件，用於超高維度的變數選擇問題。

文件連結[點此](https://cran.r-project.org/web/packages/SIS/index.html)

```{r loadSIS, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
library(SIS)
```

以套件 **SIS** 內建的 `leukemia.train` 資料作為示範例，該資料共有 38 筆資料，及 7129 個解釋變數（`V1` ~ `V7129`），欄位 `V7130` 為二元反應變數。

```{r leukemiaData, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Sure independence screening
data(leukemia.train)
# response vatiable: V7130
dim(leukemia.train)
# [1]   38 7130
```

若直接以 `glm` 建立 Logistic Regression，若不注意，函數並不會回傳任何有關維度過大的警告訊息，且最多只會輸出同樣本數數量的參數估計結果，且估計值通常極不穩定，無法從模型解讀出太多資訊。

```{r leukemiaFitGLM, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# WRONG: use glm to fit the ultrahigh dimensional leukemia data
glm_lkm <- glm(V7130 ~ ., data = leukemia.train, family = binomial(link = "logit"))
# summary(glm_lkm)
dim(summary(glm_lkm)$coefficients)
# [1] 38  4
head(summary(glm_lkm)$coefficients, 6)
```

若遇到超高維度的分析問題，可先嘗試使用 SIS 對分析資料進行降維，以下為使用 SIS 分析 `leukemia.train` 資料的示範程式。

將解釋變數整理成 `model.matrix` 格式，並指定反應變數向量。

```{r leukemiaFitSIS, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# SIS method
lkm_x <- model.matrix(V7130 ~ ., data = leukemia.train)[,-1]
lkm_y <- leukemia.train[,7130]
sis_lkm <- SIS(lkm_x, lkm_y, family = "binomial", penalty = "lasso",
               tune = "bic", seed = 1) #, varISIS = "vanilla"
# SIS selected
colnames(lkm_x)[sis_lkm$ix]
```

根據 SIS 結果，僅從 `leukemia.train` 資料中解析出兩個重要變數 `r colnames(lkm_x)[sis_lkm$ix[1]]` 及 `r colnames(lkm_x)[sis_lkm$ix[2]]`，可作圖觀察此變數分別於 `y=0` 及 `y=1` 時的分布，較容易看出於 Y 不同時，此兩變數的數值分布具有視覺上的差異。

```{r leukemiaFitSISPlot, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
lkm_x_sis <- lkm_x[,sis_lkm$ix]
ggpairs(data.frame(cbind(lkm_x_sis, y = lkm_y)),
        aes(colour = as.factor(lkm_y), alpha = 0.4))
```

此時，再以 `glm` 函數只針對兩個重要變數 `V3320` 及 `V3810` 建立 Logistic Regression。

```{r leukemiaFitReducedGLM, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
leukemia_sis <- data.frame(cbind(lkm_x_sis, y = lkm_y))
summary(glm(y ~ ., data = leukemia_sis, family = binomial(link = "logit")))
```

## 參考文獻

- Fan, J., & Lv , J. (2008). Sure independence screening for ultrahigh dimensional feature space. *Journal of the Royal Statistical Society Series B: Statistical Methodology*, **70(5)**, 849 - 911.
- Simon, N., Friedman, J., Hastie, T., & Tibshirani , R. (2013). A sparse group lasso. *Journal of computational and graphical statistics*, 22(2), 231 - 245.
- Yuan, M. and L. Lin (2006), Model Selection and Estimation in Regression with Grouped Variables, *Journal of the Royal Statistical Society, Series B: Statistical Methodology*, **68**, 49 - 67.
- Zou, H., & Hastie, T. (2005). Regularization and variable selection via the elastic net. *Journal of the Royal Statistical Society Series B: Statistical Methodology*, **67(2)**, 301 - 320.

# Homework: Lasso Tool Comparison

- 比較 R 的 glmnet 與 python 的 Lasso 建模之差異

作業內容：

1. 使用課堂的信用卡資料。

```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
'https://raw.githubusercontent.com/PingYangChen/BDA_Course_R_Code/refs/heads/main/sample_data/Credit.csv'
```

2. 使用 R **glmnet** 套件建立 Lasso 迴歸。
3. 使用 python **scikit-learn** 套件建立 Lasso 迴歸。
4. 比較與說明 R **glmnet** 以及 python **scikit-learn** 的模型參數估計結果。
5. 自行找一份資料維度較大的資料，試作步驟 2. - 4.，比較兩種功能相同工具的模型參數估計結果。
6. 比較與說明 R **glmnet** 以及 python **scikit-learn** 在使用上的差異，含前置作業、執行步驟等。


### 信用卡資料 Lasso 範例 Python Code 

- Python 環境設定[參考連結](https://html-preview.github.io/?url=https://github.com/PingYangChen/DS-pytutorial/blob/main/lecture_note/python_basic.html)
- **scikit-learn**[文件連結](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV)


```{python, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler  # 用於進行資料標準化
from sklearn.linear_model import LassoCV  # 用於進行 Lasso 回歸的交叉驗證
```

讀取信用卡資料集，並對其進行資料的預處理，將目標變數設定為 `Balance`（信用卡餘額），並轉換所有的分類變數為虛擬變數，以便在後續的模型分析中使用。
```{python, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# 從網路讀取信用卡資料集並載入為 pandas DataFrame
creDf = pd.read_csv('https://raw.githubusercontent.com/PingYangChen/DS-pytutorial/refs/heads/main/sample_data/Credit.csv')
# 將 'Own', 'Student', 'Married' 和 'Region' 這些分類變數轉換為虛擬變數，並刪除第一個類別以避免多重共線性
creDfDummy = pd.get_dummies(creDf, columns=['Own', 'Student', 'Married', 'Region'], drop_first=True, dtype=int)
# 設定目標變數 y 為 'Balance'（信用卡餘額）
y = creDfDummy['Balance']
# 設定自變數 x ，為所有欄位（不包含 'Balance' 欄位）
x = creDfDummy[np.setdiff1d(creDfDummy.columns, 'Balance').tolist()]
print(x.columns)
```

首先對資料進行標準化，這是正規化迴歸的一個前處理步驟。
```{python, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# 對自變數進行標準化，使每個變數的平均值為 0，標準差為 1
scaler = StandardScaler()
scaler.fit(x)
x_standard = scaler.transform(x)
```

```{python, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# 設定一組 alpha 值，用於進行交叉驗證
alpha_vec = 2**np.linspace(-5, 5, 100)
```

使用 `Lasso` 回歸進行交叉驗證，選擇最佳的懲罰參數 `alpha`，並顯示變數的迴歸係數。
```{python, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# 使用 Lasso 回歸進行交叉驗證，選擇最合適的 alpha 值
lasso = LassoCV(alphas=alpha_vec, fit_intercept=True, cv=5)
lasso.fit(x_standard, y)
lasso.intercept_ # 模型的截距項
# 將 Lasso 模型的迴歸係數轉換為 pandas DataFrame 格式並顯示
lasso_coef = pd.DataFrame({'var': x.columns, 'coef': lasso.coef_})
print(lasso_coef)
```


